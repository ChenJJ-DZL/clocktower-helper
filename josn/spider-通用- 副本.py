import os
import time
import json
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================= é…ç½®åŒºåŸŸ =================
BASE_URL = "https://clocktower-wiki.gstonegames.com"
# ä½ æƒ³æŠ“å–çš„å‰§æœ¬ç½‘å€
START_URL = "https://clocktower-wiki.gstonegames.com/index.php?title=%E4%BC%A0%E5%A5%87%E8%A7%92%E8%89%B2"
OUTPUT_FILENAME = "blood_clocktower_æ‰€æœ‰ä¼ å¥‡è§’è‰².json" # ä¿å­˜çš„æ–‡ä»¶å

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Referer": BASE_URL
}
# ===========================================

# åˆ›å»ºå¸¦æœ‰é‡è¯•åŠŸèƒ½çš„ Session
session = requests.Session()
retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
session.mount('http://', HTTPAdapter(max_retries=retries))
session.mount('https://', HTTPAdapter(max_retries=retries))
session.headers.update(HEADERS)

def get_page(url):
    try:
        response = session.get(url, timeout=15, verify=False)
        response.raise_for_status()
        return response
    except Exception as e:
        print(f"âš ï¸ è¿æ¥æ³¢åŠ¨ï¼Œé‡è¯•ä¸­... ({e})")
        time.sleep(2)
        try:
            response = session.get(url, timeout=20, verify=False)
            response.raise_for_status()
            return response
        except Exception:
            return None

def clean_soup(soup):
    """æ¸…ç†å¤šä½™æ ‡ç­¾"""
    selectors = ['#toc', '.mw-editsection', 'script', 'style', 
                 '#catlinks', '#footer', '.printfooter', '#mw-navigation', '.magnify']
    for sel in selectors:
        for tag in soup.select(sel):
            tag.decompose()

def extract_text_recursive(element):
    """
    é€’å½’æå–æ–‡æœ¬ï¼Œä¿ç•™ç»“æ„ä¸Šçš„æ¢è¡Œï¼Œä½†ä¸ä¿ç•™HTMLæ ‡ç­¾ã€‚
    è¿”å›ä¸€ä¸ªçº¯æ–‡æœ¬å­—ç¬¦ä¸²ã€‚
    """
    if element.name is None:
        return str(element)
    
    # æ ‡é¢˜åŠ æ ‡è®°ï¼Œæ–¹ä¾¿AIè¯†åˆ«ç»“æ„
    if element.name in ['h1', 'h2']:
        return f"\nã€{element.get_text(strip=True)}ã€‘\n"
    if element.name in ['h3', 'h4']:
        return f"\n[{element.get_text(strip=True)}]\n"
    
    # èŒƒä¾‹ (Pre)
    if element.name == 'pre':
        return f"\n> èŒƒä¾‹: {element.get_text(separator='', strip=True)}\n"
    
    # åˆ—è¡¨
    if element.name == 'li':
        return f"- {element.get_text(strip=True)}\n"
        
    # å®¹å™¨é€’å½’
    text_content = ""
    for child in element.children:
        text_content += extract_text_recursive(child)
    
    # æ®µè½åŠ æ¢è¡Œ
    if element.name == 'p':
        return text_content + "\n"
        
    return text_content

def main():
    print("ğŸš€ å¼€å§‹æŠ“å–æ•°æ®å¹¶è½¬æ¢ä¸º JSON æ ¼å¼...")
    
    response = get_page(START_URL)
    if not response: return

    soup = BeautifulSoup(response.content, 'html.parser')
    content_area = soup.find('div', id='mw-content-text')
    links = content_area.find_all('a')
    
    target_urls = []
    seen_titles = set()

    # 1. è·å–æ‰€æœ‰è§’è‰²é“¾æ¥
    for link in links:
        href = link.get('href')
        title = link.get('title')
        if href and title and "/index.php?title=" in href:
            if any(x in title for x in ["ç¼–è¾‘", "æ–‡ä»¶", "æ¨¡æ¿", "åˆ†ç±»", "Special"]): continue
            if title not in seen_titles:
                full_url = urljoin(BASE_URL, href)
                target_urls.append((title, full_url))
                seen_titles.add(title)

    print(f"ğŸ” å‘ç° {len(target_urls)} ä¸ªè§’è‰²ï¼Œå¼€å§‹å¤„ç†...")
    
    # 2. å‡†å¤‡æ•°æ®åˆ—è¡¨
    all_characters_data = []

    for idx, (title, url) in enumerate(target_urls):
        print(f"[{idx+1}/{len(target_urls)}] è§£æ: {title} ...")
        
        detail_resp = get_page(url)
        if detail_resp:
            page_soup = BeautifulSoup(detail_resp.content, 'html.parser')
            clean_soup(page_soup)
            
            main_content = page_soup.find('div', class_='mw-parser-output')
            h1 = page_soup.find('h1', id='firstHeading')
            final_title = h1.text.strip() if h1 else title
            
            # æå–æ­£æ–‡çº¯æ–‡æœ¬
            if main_content:
                raw_text = extract_text_recursive(main_content)
                # æ¸…æ´—ä¸€ä¸‹å¤šä½™çš„ç©ºè¡Œ
                cleaned_text = "\n".join([line.strip() for line in raw_text.splitlines() if line.strip()])
                
                # æ„å»ºå•ä¸ªè§’è‰²å¯¹è±¡
                char_data = {
                    "id": idx + 1,
                    "name": final_title,
                    "url": url,
                    "content": cleaned_text # è¿™é‡ŒåŒ…å«äº†èƒŒæ™¯ã€èƒ½åŠ›ã€èŒƒä¾‹çš„æ‰€æœ‰æ–‡æœ¬
                }
                all_characters_data.append(char_data)
            
            time.sleep(0.5)

    # 3. ä¿å­˜ä¸º JSON æ–‡ä»¶
    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
        # ensure_ascii=False ä¿è¯ä¸­æ–‡æ­£å¸¸æ˜¾ç¤ºï¼Œindent=4 ä¿è¯æ ¼å¼ç¾è§‚
        json.dump(all_characters_data, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ‰ æˆåŠŸï¼æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°å½“å‰ç›®å½•ä¸‹çš„: {OUTPUT_FILENAME}")
    print("ç°åœ¨ä½ å¯ä»¥ç›´æ¥æŠŠè¿™ä¸ªæ–‡ä»¶æ‹–è¿› Cursor äº†ã€‚")
    input("æŒ‰å›è½¦é”®é€€å‡º...")

if __name__ == "__main__":
    main()